{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating response / source nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    RelevancyEvaluator,\n",
    "    GuidelineEvaluator,\n",
    ")\n",
    "\n",
    "from llama_index.core.query_engine import (\n",
    "    FLAREInstructQueryEngine,\n",
    "    RetrieverQueryEngine,\n",
    "    RetryQueryEngine,\n",
    "    RetryGuidelineQueryEngine,\n",
    "    KnowledgeGraphQueryEngine,\n",
    ")\n",
    "\n",
    "from llama_index.core import (\n",
    "    TreeIndex,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Response,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb import Settings\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "import importlib\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=OPENAI_API_KEY,  \n",
    "    api_version=\"2024-05-01-preview\", # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    "    api_type=\"azure\", \n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.prompts.default_prompt_selectors import \\\n",
    "    DEFAULT_TREE_SUMMARIZE_PROMPT_SEL\n",
    "from llama_index.core.query_engine import (RouterQueryEngine,\n",
    "                                           TransformQueryEngine)\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.selectors import LLMMultiSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "from util.helpers import create_and_save_wiki_md_files, get_wiki_pages\n",
    "from util.query_engines import VerboseHyDEQueryTransform, WeatherQueryEngine\n",
    "\n",
    "import chromadb\n",
    "from chromadb import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./data/baseline-rag-pdf-docs/chromadb\", settings=Settings(allow_reset=True))\n",
    "chroma_client.reset()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"landsforsoeg\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=\"gpt4\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    # api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    api_version = \"2024-05-01-preview\", # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    # api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    api_version = \"2024-05-01-preview\", # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Define the ingestion pipeline to add documents to vector store\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch documents\n",
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "\n",
    "# Run pipeline\n",
    "pipeline.run(documents=documents)\n",
    "\n",
    "print(\"Indexing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=\"gpt4\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=documents, embed_model=embed_model, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt of RelevancyEvaluator\n",
    "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n",
    "    \"Your task is to evaluate if the response for the query \\\n",
    "    is in line with the context information provided.\\n\"\n",
    "    \"You have two options to answer. Either YES/ NO.\\n\"\n",
    "    \"Answer - YES, if the response for the query \\\n",
    "    is in line with context information otherwise NO.\\n\"\n",
    "    \"Query and Response: \\n {query_str}\\n\"\n",
    "    \"Context: \\n {context_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"We want to understand if the following query and response is\"\n",
    "    \"in line with the context information: \\n {query_str}\\n\"\n",
    "    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"If the existing answer was already YES, still answer YES. \"\n",
    "    \"If the information is present in the new context, answer YES. \"\n",
    "    \"Otherwise answer NO.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hvordan bekæmper jeg væselhale?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = RelevancyEvaluator(llm=llm, eval_template=DEFAULT_EVAL_TEMPLATE, refine_template=DEFAULT_REFINE_TEMPLATE)\n",
    "evaluator = RelevancyEvaluator(llm=llm)\n",
    "query_engine = RetryQueryEngine(\n",
    "    query_engine=index.as_query_engine(llm=llm), evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Query: hvordan bekæmper jeg væselhale?\n",
       "\n",
       "Response:\n",
       "\n",
       " For at bekæmpe væselhale, kan du anvende 0,7 l. Mateno Duo 600 SC pr. ha. Dette har vist sig at være mest effektivt, især når det anvendes tidligt, for eksempel den 20. august. Tilsætning af Boxer kan også reducere mængden af væselhale i frøet, men det kan ikke anvendes i praksis på de behandlingstidspunkter, der er blevet anvendt i dette forsøg, på grund af fordampning af prosulfocarb. Andre muligheder inkluderer kombinationer af Mateno Duo og Boxer i forskellige mængder, samt tilføjelse af Atlantis OD i nogle tilfælde. Det er vigtigt at bemærke, at resultaterne kan variere, og det optimale tidspunkt for anvendelse kan variere."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(str_or_query_bundle=query)\n",
    "display(Markdown(f\"Query: {query}\\n\\nResponse:\\n\\n {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import EvaluationResult\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(\n",
    "    query: str, response: Response, eval_result: EvaluationResult\n",
    ") -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "            \"Reasoning\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a99c8_row0_col1, #T_a99c8_row0_col2 {\n",
       "  inline-size: 600px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a99c8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a99c8_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_a99c8_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
       "      <th id=\"T_a99c8_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
       "      <th id=\"T_a99c8_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
       "      <th id=\"T_a99c8_level0_col4\" class=\"col_heading level0 col4\" >Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a99c8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a99c8_row0_col0\" class=\"data row0 col0\" >hvordan bekæmper jeg væselhale?</td>\n",
       "      <td id=\"T_a99c8_row0_col1\" class=\"data row0 col1\" >For at bekæmpe væselhale, kan du anvende 0,7 l. Mateno Duo 600 SC pr. ha. Dette har vist sig at være mest effektivt, især når det anvendes tidligt, for eksempel den 20. august. Tilsætning af Boxer kan også reducere mængden af væselhale i frøet, men det kan ikke anvendes i praksis på de behandlingstidspunkter, der er blevet anvendt i dette forsøg, på grund af fordampning af prosulfocarb. Andre muligheder inkluderer kombinationer af Mateno Duo og Boxer i forskellige mængder, samt tilføjelse af Atlantis OD i nogle tilfælde. Det er vigtigt at bemærke, at resultaterne kan variere, og det optimale tidspunkt for anvendelse kan variere.</td>\n",
       "      <td id=\"T_a99c8_row0_col2\" class=\"data row0 col2\" >Den bed -\n",
       "ste bekæmpelse af væselhale er opnået i led 3, hvor der \n",
       "har været anvendt 0,7 l. Mateno Duo 600 SC pr. ha den \n",
       "20. august – altså en tidlig sprøjtning. Indholdet af væ -\n",
       "selhale i frøet i led 3 er dog stadig for højt i forhold til \n",
       "certificering, men er nedbragt væsentligt sammenlignet \n",
       "med ubehandlet. Tilsætning af Boxer giver i dette forsøg \n",
       "færre væselhaler i frøet, men kan grundet fordampning \n",
       "af prosulfocarb ikke anvendes i praksis på de behand -\n",
       "lingstidspunkter, der har været anvendt i dette forsøg. \n",
       "Der har været en god bekæmpelse af alm. rapgræs i \n",
       "Slemning efter kraftig nedbør umiddelbart efter såning giver \n",
       "udfordringer med fremspiringen.FOTO: KRISTIAN JURANICH, SEGES INNOVATION...</td>\n",
       "      <td id=\"T_a99c8_row0_col3\" class=\"data row0 col3\" >Pass</td>\n",
       "      <td id=\"T_a99c8_row0_col4\" class=\"data row0 col4\" >YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fad8b464f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_response(\n",
    "    query=query, response=response\n",
    ")\n",
    "\n",
    "display_eval_df(query, response, eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
